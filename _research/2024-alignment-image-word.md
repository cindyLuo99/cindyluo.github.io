---
title: "Finding Unsupervised Alignment of Conceptual Systems in Imageâ€‘Word Representations"
collaborators: "Kexin Luo, Bei Zhang, Yajie Xiao, Brenden Lake"
year: 2024
order: 2
links:
  - label: "CogSci 2024"
    url: "/publication/2024-cogsci-alignment-image-word"
keywords: "alignment, self-supervised learning, image-word representations, concept learning, developmental headcam data"
resources:
  - label: "Paper"
    url: "https://escholarship.org/uc/item/7dz6b64q"
  - label: "Slides"
    url: "https://docs.google.com/presentation/d/1y91_b7CGSFtanWLU9g-BuNkWoY8cw57xIUl76gc4T3U/edit?usp=sharing"
---

Advancements in deep neural networks have led to significant progress in computer vision and natural language processing. These networks, trained on real-world stimuli, develop high-level feature representations of stimuli. It is hypothesized that these representations, stemming from different inputs, should converge into similar conceptual systems, as they reflect various perspectives of the same underlying reality. This paper examines the degree to which different conceptual systems can be aligned in an unsupervised manner, using feature-based representations from deep neural networks. Our investigation centers on the alignment between the image and word representations produced by diverse neural networks, emphasizing those trained via self-supervised learning methods. Subsequently, to probe comparable alignment patterns in human learning, we extend this examination to models trained on developmental headcam data from children. Our findings reveal a more pronounced alignment in models trained through self-supervised learning compared to supervised learning, effectively uncovering higher-level structural connections among categories. However, this alignment was notably absent in models trained with limited developmental headcam data, suggesting more data, more inductive biases, or more supervision are needed to establish alignment from realistic input.


